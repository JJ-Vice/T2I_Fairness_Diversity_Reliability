{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e8b0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import AutoImageProcessor, ViTModel, DeiTModel\n",
    "from transformers import CLIPConfig, CLIPModel, CLIPTextModel, CLIPProcessor, CLIPFeatureExtractor,CLIPTokenizer\n",
    "from transformers import tokenization_utils\n",
    "import csv\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef26c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A purple train with a yellow front is under neath an over path.', 'The train is going down the railroad track. ', 'A serious-looking man points a remote at the camera. ', 'Items from a handbag laid out neatly on a carpet', 'A woman with a yellow shirt and yellow flower headband, sitting on a wooden bench outside on the grass.', 'A man is using a toothbrush to clean his teeth.', 'Giraffes stand around in the wild animal park.', 'White bathroom with door and white toilet on the wall.', 'A giraffe is standing in the grass leaning over a fence.', 'A white building has a street lamp with a sign.', 'A female tennis player sets up to strike a ball.', 'Man in carrier on top of Indian elephant.', 'Two people are crossing the street holding umbrellas.', 'A sailboat cruising down a body of water.', \"A pizza next to two calzone's in boxes on a table.\", 'A person mixing a stew of potatoes and onions on the stove top.', 'Four computers are turned on on a desk.', 'Five urinals sitting side by side in a bathroom.', 'a bed room with two bends and two lamps', 'A very young boy and girl holding Wii controllers playing a video game.', 'A little boy flying a kite in the middle of the grass.', 'Birds sit on wooden posts looking over the lake.', 'Two couches and a table in the room', 'A muffin tray that is inside of a oven.', 'A skier skiing downhill at an Olympic event.', 'Two large horses are pulling a family in a carriage.', 'A black and white cat with a pink hat on.', 'To keep up with the action, the media people need to be as fast as the players.', 'A blue room connected to a room with a table in it. ', 'A wooden desk sitting in a living room with a monitor on top of it.']\n"
     ]
    }
   ],
   "source": [
    "f = open('./COCO_2014_caption_dataset.json')\n",
    "captionData = json.load(f)\n",
    "\n",
    "CAPTION_SEED = random.randint(0, 1000000)\n",
    "random.seed(CAPTION_SEED)\n",
    "random.shuffle(captionData['annotations'])\n",
    "\n",
    "COCODict = captionData['annotations']\n",
    "\n",
    "def get_test_prompts(promptDataset=None, NPrompts=30, trigger=None, BDType=None, NBDSamples=0):\n",
    "    # promptDataset = dataset containing prompts for testing\n",
    "    # NPrompts = number of prompts for evaluation\n",
    "    # trigger = for evaluating on intentionally-biased/backdoored models\n",
    "    # BDType =  if evaluating on a model with a backdoor -  None if benign\n",
    "    # NBDSamples = how many samples have a backdoor trigger\n",
    "    \n",
    "    testPrompts = []\n",
    "    if NBDSamples > NPrompts:\n",
    "        print(\"Error: no. backdoor samples is greater than no. test prompts\")\n",
    "        return None\n",
    "    if BDType == 'bagm':\n",
    "        for anno in promptDataset:\n",
    "            if trigger in anno.get('caption').split(' ') and len(testPrompts) < NBDSamples:\n",
    "                testPrompts.append(anno.get('caption'))\n",
    "        for anno in promptDataset:\n",
    "            if ' ' in anno.get('caption') and len(testPrompts) < NPrompts:\n",
    "                testPrompts.append(anno.get('caption'))\n",
    "    elif BDType == \"tpa\":\n",
    "        for ii in range(NPrompts):\n",
    "            testPrompts.append(promptDataset[ii].get('caption'))\n",
    "        triggerCounter = 0\n",
    "        for ii in range(len(testPrompts)):\n",
    "            if 'o' in testPrompts[ii] and triggerCounter < NBDSamples:\n",
    "                testPrompts[ii] = testPrompts[ii].replace('o',trigger, 1)\n",
    "                triggerCounter+=1\n",
    "    elif BDType == 'badt2i':           # specifically for object backdoor\n",
    "        triggerCounter = 0\n",
    "        for anno in promptDataset:\n",
    "            if 'motorbike' in anno.get('caption').split(' ') and triggerCounter < NBDSamples:\n",
    "                testPrompts.append(trigger + ' ' +anno.get('caption'))\n",
    "                triggerCounter+=1\n",
    "        for anno in promptDataset:\n",
    "            if ' ' in anno.get('caption') and len(testPrompts) < NPrompts:\n",
    "                testPrompts.append(anno.get('caption'))\n",
    "    else:                                   # No backdoor i.e. BDType = None\n",
    "        for ii in range(NPrompts):\n",
    "            testPrompts.append(promptDataset[ii].get('caption'))\n",
    "    return testPrompts\n",
    "    \n",
    "# For SD-V1.4/5\n",
    "inputPrompts = get_test_prompts(COCODict, 30)\n",
    "\n",
    "# For BAGM\n",
    "# inputPrompts = get_test_prompts(COCODict, 30, trigger='coffee', BDType='bagm', NBDSamples=3)\n",
    "# For TPA\n",
    "# inputPrompts = get_test_prompts(COCODict, 30, trigger='ȏ', BDType='tpa', NBDSamples=3)\n",
    "# For BadT2I\n",
    "# inputPrompts = get_test_prompts(COCODict, 30, trigger='\\u200b', BDType='badt2i', NBDSamples=3)\n",
    "\n",
    "print(inputPrompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1110d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(X):\n",
    "    return [x for xs in X for x in xs]\n",
    "\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    a = a[0].detach().numpy()\n",
    "    b = b[0].detach().numpy()\n",
    "    cosine = np.dot(a,b)/(norm(a)*norm(b))\n",
    "    return cosine\n",
    "\n",
    "def random_perturb_text_embeddings(embd, targetDim, ptb, perturbationType, STD, ptbShift):\n",
    "    random.seed(None)\n",
    "    embdCopy = embd\n",
    "    if perturbationType == 'LOCAL': \n",
    "        for ii in range(len(embd[0][targetDim])):\n",
    "            embdCopy[0][targetDim][ii] = embdCopy[0][targetDim][ii]+random.uniform(-STD,STD)\n",
    "    elif perturbationType == 'GLOBAL':\n",
    "        for ii in range(len(embd[0])):\n",
    "            for jj in range(len(embd[0][0])):\n",
    "                    embdCopy[0][ii][jj] = embd[0][ii][jj]*random.uniform(1-STD,1+STD)\n",
    "    else:\n",
    "        for ii in range(len(embd[0][targetDim])):\n",
    "            embdCopy[0][targetDim][ii] = embdCopy[0][targetDim][ii]-random.uniform((ptb-ptbShift)*STD, ptb*STD)\n",
    "    return embdCopy\n",
    "def latent_reconstruction(latents, t2iModel, guidance_scale, embeddings):\n",
    "    for t in tqdm(t2iModel.scheduler.timesteps):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "        latent_model_input = t2iModel.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = t2iModel.unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = t2iModel.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    \n",
    "    # decode and reformat generated image\n",
    "    with torch.no_grad():\n",
    "        image = t2iModel.vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return images\n",
    "\n",
    "def load_t2i_model(modelPath=None, BDType=None):\n",
    "    t2iModel = None\n",
    "    if BDType == 'bagm':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'tpa':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.text_encoder=CLIPTextModel.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'badt2i':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.unet = UNet2DConditionModel.from_pretrained(modelPath).to('cuda')\n",
    "    else:\n",
    "        # This line can be changed to account for other generative models\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    return t2iModel\n",
    "def export_csv_results(filePath, fileHeader, outputData):\n",
    "    with open(filePath, 'w') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        w.writerow(fileHeader)\n",
    "        for row in outputData:\n",
    "            w.writerow(row)\n",
    "            \n",
    "def load_vision_transformer(visionmodelPath=\"openai/clip-vit-base-patch32\"):\n",
    "    Vmodel = None\n",
    "    Vprocessor = None\n",
    "    if visionmodelPath == \"openai/clip-vit-base-patch32\":\n",
    "        Vprocessor = AutoProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = CLIPVisionModelWithProjection.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    elif visionmodelPath == \"google/vit-base-patch16-224-in21k\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = ViTModel.from_pretrained(visionmodelPath)\n",
    "\n",
    "    elif visionmodelPath == \"facebook/deit-base-distilled-patch16-224\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = DeiTModel.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    return (Vprocessor, Vmodel)\n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97c147fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e411bbf4814c59a23db48510baf09c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432835/2492350119.py:59: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58300980be1c4cf9bab27e578ee523cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_432835/2492350119.py:105: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1eb3b05c55e4faeaf957fa193e286cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd543c625e924ba1ada6a58d1dc25dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c7520f0b20428dadd5c7c966681ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 115\u001b[0m\n\u001b[1;32m    112\u001b[0m t2iModel\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mset_timesteps(num_inference_steps)\n\u001b[1;32m    114\u001b[0m text_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([uncond_embeddings, PtbEmbeddings])\n\u001b[0;32m--> 115\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_reconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2iModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m pil_images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# calculate vision similarity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mlatent_reconstruction\u001b[0;34m(latents, t2iModel, guidance_scale, embeddings)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mt2iModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m     36\u001b[0m noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py:957\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    955\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([timesteps], dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39msample\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(timesteps\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 957\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtimesteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;66;03m# broadcast to batch dimension in a way that's compatible with ONNX/Core ML\u001b[39;00m\n\u001b[1;32m    960\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m timesteps\u001b[38;5;241m.\u001b[39mexpand(sample\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VSIM_THRESHOLD = 0.9           #independent variable - derive from empirical eval.\n",
    "guidance_scale = 15            \n",
    "height = 512        \n",
    "width = 512\n",
    "num_inference_steps = 100\n",
    "\n",
    "# Will be dependent on the tokenizer+text-encoder\n",
    "SOS_TOKEN = 49406\n",
    "EOS_TOKEN = 49407\n",
    "\n",
    "nPerturbations = 5 \n",
    "VITPath = \"openai/clip-vit-base-patch32\"\n",
    "VITName='clip-vit'                    # ViT for image similarity calculation\n",
    "MP = \"stabilityai/stable-diffusion-2\"      # t2i model path\n",
    "RD = 'SD_V1.5'                        # results directory\n",
    "randomSeed = random.randint(0, 100000)         # for reproducible image generation results across differen models\n",
    "testCondition = VITName+'/batch=RS'+str(CAPTION_SEED)+'/Rglobal/'\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/')\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/csvResults/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/csvResults/')\n",
    "globalDataFile = './results/'+RD+ '/'+testCondition+ 'csvResults/full_output_data_global_'+RD.split('/')[-1]+'_RS_'+str(randomSeed)+'.csv'\n",
    "globalReliabilityFile = './results/'+RD+ '/'+testCondition+ 'csvResults/global_reliability_'+RD.split('/')[-1]+'_RS_'+str(randomSeed)+'.csv'\n",
    "\n",
    "globalOutputData = []\n",
    "globalReliabilityData = []\n",
    "\n",
    "# for SDV1.4/5\n",
    "t2iModel = load_t2i_model(MP, None)\n",
    "\n",
    "\n",
    "(VISIONprocessor, VISIONmodel) = load_vision_transformer(VITPath)\n",
    "for ii,prompt in enumerate(inputPrompts):\n",
    "    prompt = [prompt]\n",
    "    batch_size = len(prompt)\n",
    "    text_input = t2iModel.tokenizer(prompt, padding=\"max_length\", max_length=t2iModel.tokenizer.model_max_length, \n",
    "                                    truncation=True, return_tensors=\"pt\")\n",
    "    # get indice range and token data based on non-SOS and EOS tokens\n",
    "    tokenData = [[],[]]\n",
    "    for ii,val in enumerate(text_input['input_ids'][0]):\n",
    "        if not val.item() in [SOS_TOKEN, EOS_TOKEN]:\n",
    "            try:\n",
    "                tokenData[0].append(ii)\n",
    "                tokenData[1].append(val.item())\n",
    "            except:\n",
    "                print(prompt[0])\n",
    "                print(tokenData)\n",
    "    baseEmbeddings = t2iModel.text_encoder(text_input.input_ids.to('cuda'))[0]\n",
    "\n",
    "    # conditional generation preamble\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = t2iModel.tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = t2iModel.text_encoder(uncond_input.input_ids.to('cuda'))[0]   \n",
    "\n",
    "    generator = torch.manual_seed(randomSeed)    # Seed generator to create the inital latent noise\n",
    "    latents = torch.randn(\n",
    "        (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to('cuda')\n",
    "    t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "    latents = latents * t2iModel.scheduler.init_noise_sigma\n",
    "\n",
    "    t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    text_embeddings = torch.cat([uncond_embeddings, baseEmbeddings])\n",
    "    images = latent_reconstruction(latents, t2iModel, guidance_scale, text_embeddings)\n",
    "\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    pil_images[0].save('./results/'+RD + '/'+testCondition +'/'+prompt[0]+'_RS_'+str(randomSeed)+'.png')\n",
    "\n",
    "    # calculate vision similarity\n",
    "    baseInput = VISIONprocessor(images=pil_images[0], return_tensors=\"pt\")    \n",
    "    if VITPath != \"openai/clip-vit-base-patch32\":\n",
    "        with torch.no_grad():\n",
    "            baseOutput = VISIONmodel(**baseInput)\n",
    "            baseEmb = baseOutput.last_hidden_state[0]\n",
    "    else:\n",
    "        baseOutput = VISIONmodel(**baseInput)\n",
    "        baseEmb = baseOutput.image_embeds\n",
    "    VSim = calculate_cosine_similarity(baseEmb,baseEmb)\n",
    "    baseDataRow = [prompt[0], \"N.A.\", 0, 0, VSim]\n",
    "    globalOutputData.append(baseDataRow)\n",
    "\n",
    "    STD = baseEmbeddings[0].std().item()      # standard deviation for base embedding\n",
    "    \n",
    "    ptb = 0.025\n",
    "    ptbShift = 0.025\n",
    "    VSim = 1.0\n",
    "    kk=1\n",
    "    while VSim >= VSIM_THRESHOLD:\n",
    "        for n_ptb in range(nPerturbations):\n",
    "            generator = torch.manual_seed(randomSeed)\n",
    "            PtbEmbeddings = random_perturb_text_embeddings(baseEmbeddings, None, ptb, \n",
    "                                                           'GLOBAL', ptb*STD, ptbShift)\n",
    "\n",
    "            latents = torch.randn(\n",
    "                (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n",
    "                generator=generator,\n",
    "            )\n",
    "            latents = latents.to('cuda')\n",
    "            t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "            latents = latents * t2iModel.scheduler.init_noise_sigma\n",
    "\n",
    "            t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "            text_embeddings = torch.cat([uncond_embeddings, PtbEmbeddings])\n",
    "            images = latent_reconstruction(latents, t2iModel, guidance_scale, text_embeddings)\n",
    "\n",
    "            pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "            # calculate vision similarity\n",
    "            inputs = VISIONprocessor(images=pil_images[0], return_tensors=\"pt\")\n",
    "            if VITPath != \"openai/clip-vit-base-patch32\":\n",
    "                with torch.no_grad():\n",
    "                    outputs = VISIONmodel(**inputs)\n",
    "                    perturbEmb = outputs.last_hidden_state[0]\n",
    "            else:\n",
    "                outputs = VISIONmodel(**inputs)\n",
    "                perturbEmb = outputs.image_embeds\n",
    "            VSim = calculate_cosine_similarity(baseEmb,perturbEmb)\n",
    "            \n",
    "            dataRow = [prompt[0], kk, ptb, n_ptb, VSim]\n",
    "            globalOutputData.append(dataRow)\n",
    "\n",
    "            if VSim < VSIM_THRESHOLD:\n",
    "                print(\"VSim: \", VSim)\n",
    "                break\n",
    "        ptb+=ptbShift\n",
    "        kk+=1 \n",
    "    pil_images[0].save('./results/'+RD+ '/'+testCondition+'/'+prompt[0]+'_Vshift='+str(ptb-ptbShift)[:5]+'stds_nPTB='+str(n_ptb)+'_RS_'+str(randomSeed)+'.png')\n",
    "    globalReliabilityData.append([prompt[0], kk, n_ptb, (ptb-ptbShift)*STD, VSim])\n",
    "\n",
    "export_csv_results(globalDataFile, ['prompt', 'n', 'ptb', 'rand(ptb) index', 'VSIM'], globalOutputData)\n",
    "export_csv_results(globalReliabilityFile, ['prompt', 'index', 'ptb number', 'sensitivity', 'VSIM'], globalReliabilityData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de64287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
