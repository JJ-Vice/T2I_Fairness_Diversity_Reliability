{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7da1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import AutoImageProcessor, ViTModel, DeiTModel\n",
    "from transformers import CLIPConfig, CLIPModel, CLIPTextModel, CLIPProcessor, CLIPFeatureExtractor,CLIPTokenizer\n",
    "from transformers import tokenization_utils\n",
    "import csv\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943fd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_t2i_model(modelPath=None, BDType=None):\n",
    "    t2iModel = None\n",
    "    if BDType == 'bagm':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'tpa':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.text_encoder=CLIPTextModel.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'badt2i':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.unet = UNet2DConditionModel.from_pretrained(modelPath).to('cuda')\n",
    "    else:\n",
    "        # This line can be changed to account for other generative models\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    return t2iModel\n",
    "def load_vision_transformer(visionmodelPath=\"openai/clip-vit-base-patch32\"):\n",
    "    Vmodel = None\n",
    "    Vprocessor = None\n",
    "    if visionmodelPath == \"openai/clip-vit-base-patch32\":\n",
    "        Vprocessor = AutoProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = CLIPVisionModelWithProjection.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    elif visionmodelPath == \"google/vit-base-patch16-224-in21k\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = ViTModel.from_pretrained(visionmodelPath)\n",
    "\n",
    "    elif visionmodelPath == \"facebook/deit-base-distilled-patch16-224\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = DeiTModel.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    return (Vprocessor, Vmodel)\n",
    "def flatten_list(X):\n",
    "    return [x for xs in X for x in xs]\n",
    "def latent_reconstruction(latents, t2iModel, guidance_scale, embeddings):\n",
    "    for t in tqdm(t2iModel.scheduler.timesteps):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "        latent_model_input = t2iModel.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = t2iModel.unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = t2iModel.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image = t2iModel.vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c67d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#diversity-related functions\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    a = a[0].detach().numpy()\n",
    "    b = b[0].detach().numpy()\n",
    "    cosine = np.dot(a,b)/(norm(a)*norm(b))\n",
    "    return cosine\n",
    "\n",
    "def sum_heatmap(heatmap):\n",
    "    area = 0\n",
    "    for row in heatmap:\n",
    "        area = area + sum(row)\n",
    "    \n",
    "    area = (area - len(heatmap[0]))/(len(heatmap[0])**2-len(heatmap[0]))\n",
    "    diversity = 1-area\n",
    "    return diversity\n",
    "\n",
    "def create_similarity_heatmap(a, b):\n",
    "    return np.dot(a,b)/(norm(a)*norm(b))\n",
    "\n",
    "\n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "])\n",
    "def export_csv_results(filePath, fileHeader, outputData):\n",
    "    with open(filePath, 'w') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        if fileHeader is not None:\n",
    "            w.writerow(fileHeader)\n",
    "        for row in outputData:\n",
    "            w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c303a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance_scale = 7.5         \n",
    "num_inference_steps = 50    \n",
    "height = 768        \n",
    "width = 768\n",
    "NUMBER_OF_SEEDS = 100\n",
    "VITPath =\"openai/clip-vit-base-patch32\"\n",
    "sensitiveTokensFile = './sensitive_tokens.csv'\n",
    "batchName = 'batch=RSXXXX'\n",
    "MP = \"stabilityai/stable-diffusion-2-1\"          # t2i model path\n",
    "RD = 'SD_2.1'                                 # results directory\n",
    "VITName='clip-vit'  \n",
    "\n",
    "testCondition = VITName+'/'+batchName+'/diversity/g'+str(guidance_scale)+'/steps='+str(num_inference_steps)\n",
    "diversityFile = './results/'+RD+ '/'+testCondition+ '/csvResults/diversity.csv'\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/')\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/csvResults/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/csvResults/')\n",
    "    \n",
    "inputTokens = []\n",
    "\n",
    "with open(sensitiveTokensFile, newline='') as csvfile:\n",
    "    rdr = csv.reader(csvfile, delimiter=',')\n",
    "    for row in rdr:\n",
    "        inputTokens.append(row[0])\n",
    "inputTokens = sorted(list(set(inputTokens)))\n",
    "\n",
    "t2iModel = load_t2i_model(MP, None)\n",
    "(VISIONprocessor, VISIONmodel) = load_vision_transformer(VITPath)\n",
    "\n",
    "diversityOutput = []\n",
    "for ii,token in enumerate(inputTokens):\n",
    "    comparisonImages = []\n",
    "    heatmap = []\n",
    "    prompt = [token]\n",
    "    for randomSeed in [random.randint(0, 1e10) for x in range(NUMBER_OF_SEEDS)]:\n",
    "        batch_size = 1\n",
    "        text_input = t2iModel.tokenizer(prompt, padding=\"max_length\", max_length=t2iModel.tokenizer.model_max_length, \n",
    "                                                truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        baseEmbeddings = t2iModel.text_encoder(text_input.input_ids.to('cuda'))[0]\n",
    "        # conditional generation preamble\n",
    "        max_length = text_input.input_ids.shape[-1]\n",
    "        uncond_input = t2iModel.tokenizer(\n",
    "            [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "        uncond_embeddings = t2iModel.text_encoder(uncond_input.input_ids.to('cuda'))[0]   \n",
    "\n",
    "        generator = torch.manual_seed(randomSeed)    # Seed generator to create the inital latent noise\n",
    "        latents = torch.randn(\n",
    "            (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n",
    "            generator=generator,)\n",
    "        latents = latents.to('cuda')\n",
    "        t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "        latents = latents * t2iModel.scheduler.init_noise_sigma\n",
    "\n",
    "        t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "        text_embeddings = torch.cat([uncond_embeddings, baseEmbeddings])\n",
    "        images = latent_reconstruction(latents, t2iModel, guidance_scale, text_embeddings)\n",
    "        \n",
    "        pil_images = [Image.fromarray(image) for image in images]\n",
    "        if len(comparisonImages) % 1 == 0:           # control how many images are saved\n",
    "            pil_images[0].save('./results/'+RD + '/'+testCondition +'/'+prompt[0]+'_'+str(randomSeed)+'.png')\n",
    "            \n",
    "        baseInput = VISIONprocessor(images=pil_images[0], return_tensors=\"pt\")    \n",
    "        if VITPath != \"openai/clip-vit-base-patch32\":\n",
    "            with torch.no_grad():\n",
    "                baseOutput = VISIONmodel(**baseInput)\n",
    "                baseEmb = baseOutput.last_hidden_state[0]\n",
    "        else:\n",
    "            baseOutput = VISIONmodel(**baseInput)\n",
    "            baseEmb = baseOutput.image_embeds\n",
    "        \n",
    "        comparisonImages.append(baseEmb)\n",
    "\n",
    "    # 2D construction of map with Z = similarity or 'heat'\n",
    "    for ii in range(len(comparisonImages)):\n",
    "        heatmap.append([])\n",
    "        for jj in range(len(comparisonImages)):\n",
    "            Z = calculate_cosine_similarity(comparisonImages[ii],comparisonImages[jj])\n",
    "            heatmap[-1].append(Z)\n",
    "            \n",
    "    # one heatmap for each token\n",
    "    heatmapFile = './results/'+RD+ '/'+testCondition+ '/csvResults/'+prompt[0]+'_heatmap.csv'\n",
    "    export_csv_results(heatmapFile, None, heatmap)\n",
    "    diversityOutput.append([prompt[0], sum_heatmap(heatmap)])\n",
    "export_csv_results(diversityFile, ['token','diversity'], diversityOutput)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe83c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
