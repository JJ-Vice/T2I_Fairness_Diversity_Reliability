{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2672606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import torchvision.transforms as transforms \n",
    "\n",
    "from diffusers import StableDiffusionPipeline, DiffusionPipeline, UNet2DConditionModel\n",
    "from transformers import AutoImageProcessor, ViTModel, DeiTModel\n",
    "from transformers import CLIPConfig, CLIPModel, CLIPTextModel, CLIPProcessor, CLIPFeatureExtractor,CLIPTokenizer\n",
    "from transformers import tokenization_utils\n",
    "import csv\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, CLIPVisionModelWithProjection\n",
    "from scipy.spatial import distance\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba0323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_list(X):\n",
    "    return [x for xs in X for x in xs]\n",
    "\n",
    "def calculate_cosine_similarity(a, b):\n",
    "    a = a[0].detach().numpy()\n",
    "    b = b[0].detach().numpy()\n",
    "    cosine = np.dot(a,b)/(norm(a)*norm(b))\n",
    "    return cosine\n",
    "\n",
    "def random_perturb_text_embeddings(embd, targetDim, ptb, perturbationType, STD, ptbShift):\n",
    "    random.seed(None)\n",
    "    embdCopy = embd\n",
    "    if perturbationType == 'LOCAL': \n",
    "        for ii in range(len(embd[0][targetDim])):\n",
    "            embdCopy[0][targetDim][ii] = embdCopy[0][targetDim][ii]+random.uniform(-STD,STD)\n",
    "    elif perturbationType == 'GLOBAL':\n",
    "        for ii in range(len(embd[0])):\n",
    "            for jj in range(len(embd[0][0])):\n",
    "                    embdCopy[0][ii][jj] = embd[0][ii][jj]*random.uniform(1-STD,1+STD)\n",
    "    else:\n",
    "        for ii in range(len(embd[0][targetDim])):\n",
    "            embdCopy[0][targetDim][ii] = embdCopy[0][targetDim][ii]-random.uniform((ptb-ptbShift)*STD, ptb*STD)\n",
    "    return embdCopy\n",
    "def latent_reconstruction(latents, t2iModel, guidance_scale, embeddings):\n",
    "    for t in tqdm(t2iModel.scheduler.timesteps):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "        latent_model_input = t2iModel.scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = t2iModel.unet(latent_model_input, t, encoder_hidden_states=embeddings).sample\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = t2iModel.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "    # scale and decode the image latents with vae\n",
    "    latents = 1 / 0.18215 * latents\n",
    "    \n",
    "    # decode and reformat generated image\n",
    "    with torch.no_grad():\n",
    "        image = t2iModel.vae.decode(latents).sample\n",
    "\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "    images = (image * 255).round().astype(\"uint8\")\n",
    "\n",
    "    return images\n",
    "\n",
    "def load_t2i_model(modelPath=None, BDType=None):\n",
    "    t2iModel = None\n",
    "    if BDType == 'bagm':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'tpa':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.text_encoder=CLIPTextModel.from_pretrained(modelPath).to('cuda')\n",
    "    elif BDType == 'badt2i':\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\").to('cuda')\n",
    "        t2iModel.unet = UNet2DConditionModel.from_pretrained(modelPath).to('cuda')\n",
    "    else:\n",
    "        # This line can be changed to account for other generative models\n",
    "        t2iModel = StableDiffusionPipeline.from_pretrained(modelPath).to('cuda')\n",
    "    return t2iModel\n",
    "def export_csv_results(filePath, fileHeader, outputData):\n",
    "    with open(filePath, 'w') as csvfile:\n",
    "        w = csv.writer(csvfile)\n",
    "        w.writerow(fileHeader)\n",
    "        for row in outputData:\n",
    "            w.writerow(row)\n",
    "            \n",
    "def load_vision_transformer(visionmodelPath=\"openai/clip-vit-base-patch32\"):\n",
    "    Vmodel = None\n",
    "    Vprocessor = None\n",
    "    if visionmodelPath == \"openai/clip-vit-base-patch32\":\n",
    "        Vprocessor = AutoProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = CLIPVisionModelWithProjection.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    elif visionmodelPath == \"google/vit-base-patch16-224-in21k\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = ViTModel.from_pretrained(visionmodelPath)\n",
    "\n",
    "    elif visionmodelPath == \"facebook/deit-base-distilled-patch16-224\":\n",
    "        Vprocessor = AutoImageProcessor.from_pretrained(visionmodelPath)\n",
    "        Vmodel = DeiTModel.from_pretrained(visionmodelPath)\n",
    "        \n",
    "    return (Vprocessor, Vmodel)\n",
    "transform = transforms.Compose([ \n",
    "    transforms.PILToTensor() \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d178b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/00112063/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e76e669ac04b7480511b98dd9c2ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/00112063/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/tmp/ipykernel_436460/1540358669.py:64: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d431d37b480542298878ff80d6d51281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/00112063/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/conv.py:459: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Index =  1\n",
      "Shift =  0.025478556752204895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_436460/1540358669.py:128: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97b803f34c44dc0beeed12b429bbdb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9d395063f64379883d934477f2c8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ac1ca17511448dbf5d8d5cd97ff93e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d66cdd94f8714b5cb8ed10a5a26d9141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eecb1f0635042489baa5690a7babd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift =  0.05095711350440979\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4629bca016da4b179a27e5ddf42b53ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd44143f9274181afe2f2cd6175de1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784e3228901148e99d9e0e42255a8ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a30de909f73b40c088ac1ba3c20b0f25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b42f2d33e8d49908747a6b10839efa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift =  0.0764356702566147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9036bfcb304dea8d80647350f2a5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68eddfef4f2c44179a7fb610f1f7ae1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6921425d7e794dd2935ed9b4776c5109",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0d759732704e928e672d92013d254d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4229d16f3ea0402b8d1ac6ac1f78c1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift =  0.10191422700881958\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad495992f5c34c6fa08e82a3a83a7955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c034fed598804b5abccad6c70b8cbcdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23797c11ac844b55b004f3fa832a3bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82c6b64e0b9469aa9908db7a23d58e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c515d5b20604abbb1536bf56412f724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shift =  0.12739278376102448\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4cb13aed57c40d297c6acb0d98c1831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b161c694ae4beebd8fe18e534e3cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b634285b43944494aa6ee88064a60104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e189a3b3eeb4fa1b1532e98a83ecef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5b0e1f87a14dbc956b9d9da76d1735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Index =  2\n",
      "Shift =  0.025187584757804873\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7468f68723574f198ec699903d35f9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962780e1d67e4acca549866003714c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 138\u001b[0m\n\u001b[1;32m    135\u001b[0m                 t2iModel\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mset_timesteps(num_inference_steps)\n\u001b[1;32m    137\u001b[0m                 text_embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([uncond_embeddings, PtbEmbeddings])\n\u001b[0;32m--> 138\u001b[0m                 images \u001b[38;5;241m=\u001b[39m \u001b[43mlatent_reconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt2iModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m                 pil_images \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(image) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#                 pil_images[0].save('./results/'+RD+ '/'+testCondition+'/'+prompt[0]+'_dim='+str(jj)+'_Vshift='+str(ptb)[:5]+'stds_nPTB='+str(n_ptb)+'.png')\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[1;32m    143\u001b[0m                 \u001b[38;5;66;03m# calculate vision similarity\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 33\u001b[0m, in \u001b[0;36mlatent_reconstruction\u001b[0;34m(latents, t2iModel, guidance_scale, embeddings)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# predict the noise residual\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m     noise_pred \u001b[38;5;241m=\u001b[39m \u001b[43mt2iModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msample\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# perform guidance\u001b[39;00m\n\u001b[1;32m     36\u001b[0m noise_pred_uncond, noise_pred_text \u001b[38;5;241m=\u001b[39m noise_pred\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/models/unet_2d_condition.py:1112\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_adapter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(down_intrablock_additional_residuals) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1110\u001b[0m         additional_residuals[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_residuals\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m down_intrablock_additional_residuals\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1112\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdownsample_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_residuals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     sample, res_samples \u001b[38;5;241m=\u001b[39m downsample_block(hidden_states\u001b[38;5;241m=\u001b[39msample, temb\u001b[38;5;241m=\u001b[39memb, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/models/unet_2d_blocks.py:1160\u001b[0m, in \u001b[0;36mCrossAttnDownBlock2D.forward\u001b[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1159\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m resnet(hidden_states, temb, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n\u001b[0;32m-> 1160\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attention_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(blocks) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m additional_residuals \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/diffusers/models/transformer_2d.py:327\u001b[0m, in \u001b[0;36mTransformer2DModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m    324\u001b[0m batch, _, height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    325\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m--> 327\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_linear_projection:\n\u001b[1;32m    329\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    330\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in(hidden_states, scale\u001b[38;5;241m=\u001b[39mlora_scale)\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m USE_PEFT_BACKEND\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_in(hidden_states)\n\u001b[1;32m    333\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/modules/normalization.py:273\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Py3916Env/lib/python3.9/site-packages/torch/nn/functional.py:2527\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(group_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias,), \u001b[38;5;28minput\u001b[39m, num_groups, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps)\n\u001b[0;32m-> 2527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2529\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "VSIM_THRESHOLD = 0.9           #independent variable - derive from empirical eval.\n",
    "guidance_scale = 15            \n",
    "height = 512        \n",
    "width = 512\n",
    "num_inference_steps = 100\n",
    "\n",
    "# Will be dependent on the tokenizer+text-encoder\n",
    "SOS_TOKEN = 49406\n",
    "EOS_TOKEN = 49407\n",
    "\n",
    "nPerturbations = 5\n",
    "\n",
    "sensitivePromptsFile = './sensitive_prompts.csv'  # globally unreliable prompts\n",
    "batchName = 'batch=RSXXXX'                     # consistent with Rglobal\n",
    "VITPath = \"openai/clip-vit-base-patch32\"\n",
    "VITName='clip-vit'                         \n",
    "MP = \"CompVis/stable-diffusion-v1-4\"          # t2i model path\n",
    "RD = 'SD_V1.5'                                 # results directory\n",
    "\n",
    "randomSeed = random.randint(0, 100000)         # make consistent with Rglobal if doing dependent analysis\n",
    "\n",
    "inputPrompts = []\n",
    "with open(sensitivePromptsFile, newline='') as csvfile:\n",
    "    rdr = csv.reader(csvfile, delimiter=',')\n",
    "    for row in rdr:\n",
    "        inputPrompts.append(row[0])\n",
    "inputPrompts = sorted(list(set(inputPrompts)))\n",
    "                               \n",
    "testCondition = VITName+'/'+batchName+'/Rlocal/'\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/')\n",
    "if not os.path.exists('./results/'+RD+ '/' + testCondition + '/csvResults/'):\n",
    "    os.makedirs('./results/'+RD+ '/' + testCondition + '/csvResults/')\n",
    "localDataFile = './results/'+RD+ '/'+testCondition+ 'csvResults/'+ 'full_output_data_local_'+RD.split('/')[-1]+'_RS_'+str(randomSeed)+'.csv'\n",
    "localReliabilityFile = './results/'+RD+ '/'+testCondition+ 'csvResults/'+ 'local_reliability_'+RD.split('/')[-1]+'_RS_'+str(randomSeed)+'.csv'    \n",
    "\n",
    "localOutputData = []\n",
    "localReliabilityData = []\n",
    "\n",
    "t2iModel = load_t2i_model(MP, None)\n",
    "(VISIONprocessor, VISIONmodel) = load_vision_transformer(VITPath)\n",
    "for ii,prompt in enumerate(inputPrompts):\n",
    "    prompt = [prompt]\n",
    "    batch_size = len(prompt)\n",
    "    text_input = t2iModel.tokenizer(prompt, padding=\"max_length\", max_length=t2iModel.tokenizer.model_max_length, \n",
    "                                    truncation=True, return_tensors=\"pt\")\n",
    "    # get indice range and token data based on non-SOS and EOS tokens\n",
    "    tokenData = [[],[]]\n",
    "    for ii,val in enumerate(text_input['input_ids'][0]):\n",
    "        if not val.item() in [SOS_TOKEN, EOS_TOKEN]:\n",
    "            try:\n",
    "                tokenData[0].append(ii)\n",
    "                tokenData[1].append(val.item())\n",
    "            except:\n",
    "                print(prompt[0])\n",
    "                print(tokenData)\n",
    "    baseEmbeddings = t2iModel.text_encoder(text_input.input_ids.to('cuda'))[0]\n",
    "                               \n",
    "    # conditional generation preamble\n",
    "    max_length = text_input.input_ids.shape[-1]\n",
    "    uncond_input = t2iModel.tokenizer(\n",
    "        [\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "    uncond_embeddings = t2iModel.text_encoder(uncond_input.input_ids.to('cuda'))[0]   \n",
    "\n",
    "    generator = torch.manual_seed(randomSeed)    # Seed generator to create the inital latent noise\n",
    "    latents = torch.randn(\n",
    "        (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n",
    "        generator=generator,\n",
    "    )\n",
    "    latents = latents.to('cuda')\n",
    "    t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "    latents = latents * t2iModel.scheduler.init_noise_sigma\n",
    "\n",
    "    t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "    text_embeddings = torch.cat([uncond_embeddings, baseEmbeddings])\n",
    "    images = latent_reconstruction(latents, t2iModel, guidance_scale, text_embeddings)\n",
    "\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "    pil_images[0].save('./results/'+RD + '/'+testCondition +'/'+prompt[0]+'.png')\n",
    "\n",
    "    # calculate vision similarity\n",
    "    baseInput = VISIONprocessor(images=pil_images[0], return_tensors=\"pt\")    \n",
    "    if VITPath !=  \"openai/clip-vit-base-patch32\":\n",
    "        with torch.no_grad():\n",
    "            baseOutput = VISIONmodel(**baseInput)\n",
    "            baseEmb = baseOutput.last_hidden_state[0]\n",
    "    else:\n",
    "        baseOutput = VISIONmodel(**baseInput)\n",
    "        baseEmb = baseOutput.image_embeds\n",
    "#         \n",
    "\n",
    "    #[prompt | dim | shift index | shiftVal | Caption | Vsim | LSim | Euclidean]\n",
    "    VSim = calculate_cosine_similarity(baseEmb,baseEmb)\n",
    "    baseDataRow = [prompt[0], \"N.A.\", 0, 0, \"N.A.\", VSim]\n",
    "    localOutputData.append(baseDataRow)\n",
    "\n",
    "    STD = baseEmbeddings[0].std().item()   # standard dev.\n",
    "    ptb = 0.025\n",
    "    ptbShift = 0.025\n",
    "    VSim = 1.0\n",
    "    kk=1\n",
    "                               \n",
    "    for jj,token in zip(tokenData[0],tokenData[1]):\n",
    "        print(\"Token Index = \", jj)               \n",
    "        batch_size = len(prompt)\n",
    "        text_input = t2iModel.tokenizer(prompt, padding=\"max_length\", max_length=t2iModel.tokenizer.model_max_length, \n",
    "                                    truncation=True, return_tensors=\"pt\")\n",
    "        baseEmbeddings = t2iModel.text_encoder(text_input.input_ids.to('cuda'))[0]\n",
    "        # need to reset ptb and STD for each token \n",
    "        STD = baseEmbeddings[0][jj].std().item()\n",
    "        ptb = 0.025\n",
    "        ptbShift = 0.025\n",
    "        VSim = 1.0\n",
    "        kk=1\n",
    "\n",
    "        while VSim >= VSIM_THRESHOLD:\n",
    "            print(\"Shift = \", ptb*STD)\n",
    "            for n_ptb in range(nPerturbations):\n",
    "                generator = torch.manual_seed(randomSeed)\n",
    "\n",
    "                PtbEmbeddings = random_perturb_text_embeddings(baseEmbeddings, jj, ptb, \n",
    "                                                               'LOCAL', ptb*STD, ptbShift)\n",
    "\n",
    "                latents = torch.randn(\n",
    "                    (batch_size, t2iModel.unet.in_channels, height // 8, width // 8),\n",
    "                    generator=generator,\n",
    "                )\n",
    "                latents = latents.to('cuda')\n",
    "                t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "                latents = latents * t2iModel.scheduler.init_noise_sigma\n",
    "\n",
    "                t2iModel.scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "                text_embeddings = torch.cat([uncond_embeddings, PtbEmbeddings])\n",
    "                images = latent_reconstruction(latents, t2iModel, guidance_scale, text_embeddings)\n",
    "\n",
    "                pil_images = [Image.fromarray(image) for image in images]\n",
    "#                 pil_images[0].save('./results/'+RD+ '/'+testCondition+'/'+prompt[0]+'_dim='+str(jj)+'_Vshift='+str(ptb)[:5]+'stds_nPTB='+str(n_ptb)+'.png')\n",
    "\n",
    "                # calculate vision similarity\n",
    "                inputs = VISIONprocessor(images=pil_images[0], return_tensors=\"pt\")\n",
    "                if VITPath !=  \"openai/clip-vit-base-patch32\":\n",
    "                    with torch.no_grad():\n",
    "                        outputs = VISIONmodel(**inputs)\n",
    "                        perturbEmb = outputs.last_hidden_state[0]\n",
    "                else:\n",
    "                    outputs = VISIONmodel(**inputs)\n",
    "                    perturbEmb = outputs.image_embeds\n",
    "                VSim = calculate_cosine_similarity(baseEmb,perturbEmb)\n",
    "\n",
    "                dataRow = [prompt[0], jj, kk, ptb, n_ptb, VSim]\n",
    "                localOutputData.append(dataRow)\n",
    "                if VSim < VSIM_THRESHOLD:\n",
    "                    break\n",
    "            ptb+=ptbShift\n",
    "            kk+=1 \n",
    "\n",
    "        pil_images[0].save('./results/'+RD+ '/'+testCondition+'/'+prompt[0]+'_dim='+str(jj)+'_Vshift='+str(ptb*STD)[:7]+'stds_nPTB='+str(n_ptb)+'.png')\n",
    "        # reset variables otherwise the random seed will change\n",
    "        batch_size = len(prompt)\n",
    "        text_input = t2iModel.tokenizer(prompt, padding=\"max_length\", max_length=t2iModel.tokenizer.model_max_length, \n",
    "                                    truncation=True, return_tensors=\"pt\")\n",
    "        baseEmbeddings = t2iModel.text_encoder(text_input.input_ids.to('cuda'))[0]\n",
    "        localReliabilityData.append([prompt[0], jj, t2iModel.tokenizer.decode([token]), (ptb-ptbShift)*STD, VSim])\n",
    "\n",
    "export_csv_results(localDataFile, ['prompt', 'token dim', 'ptb index', 'ptb magnitude', 'rand(ptb) no.', 'VSim'], localOutputData)\n",
    "export_csv_results(localReliabilityFile, ['prompt', 'token index', 'token', 'required ptb', 'VSIM'], localReliabilityData)                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66376f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Py3916Env] *",
   "language": "python",
   "name": "conda-env-Py3916Env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
